{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Character_Level_RNN_Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharan11/Intro-to-Deep-Learning-with-Pytorch-Exercise-Solutions/blob/master/Character_Level_RNN_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8aL0bhW8Wpm",
        "colab_type": "text"
      },
      "source": [
        "# Character-Level LSTM in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjxMpOwQm8B8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPBvjAdA8hbk",
        "colab_type": "text"
      },
      "source": [
        "Load in Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdJZIwWH8gyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# open text file and read in data as `text`\n",
        "with open('anna.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEj7x-cY83fj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "0714e29e-efc8-4dc3-90fd-96a98ac3a214"
      },
      "source": [
        "text[:300]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverything was in confusion in the Oblonskys' house. The wife had\\ndiscovered that the husband was carrying on an intrigue with a French\\ngirl, who had been a governess in their family, and she had announced to\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWle9_1s9d41",
        "colab_type": "text"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqZnKnZK9LAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay_PEIRh96tl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "ad93b621-ccef-44bd-c882-9c1dd7382c1b"
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([34, 54, 25, 18, 81, 74, 70, 23, 71, 58, 58, 58, 12, 25, 18, 18, 59,\n",
              "       23, 72, 25,  1, 31, 48, 31, 74, 62, 23, 25, 70, 74, 23, 25, 48, 48,\n",
              "       23, 25, 48, 31, 75, 74, 47, 23, 74, 26, 74, 70, 59, 23, 43, 44, 54,\n",
              "       25, 18, 18, 59, 23, 72, 25,  1, 31, 48, 59, 23, 31, 62, 23, 43, 44,\n",
              "       54, 25, 18, 18, 59, 23, 31, 44, 23, 31, 81, 62, 23, 19, 46, 44, 58,\n",
              "       46, 25, 59, 73, 58, 58, 28, 26, 74, 70, 59, 81, 54, 31, 44])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4z8J78k-6NU",
        "colab_type": "text"
      },
      "source": [
        "Pre-processing the data\n",
        "\n",
        "one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO4QUfFt98Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcXi9jgHCDUr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b19f25de-827a-4435-8aeb-cd9e3a8445fb"
      },
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bEu0RPnyUNn",
        "colab_type": "text"
      },
      "source": [
        "Creating Batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwaMsQ7-yXIa",
        "colab_type": "text"
      },
      "source": [
        "**TODO**: Write the code for creating batches in the function below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29DyJhgtCmj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8rxE9v5Ta4F",
        "colab_type": "text"
      },
      "source": [
        "Test your implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4zzeGGcTVKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqa6kOwXTeop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "64447e92-f678-4a41-dd4c-b73b9c05bab3"
      },
      "source": [
        "\n",
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[34 54 25 18 81 74 70 23 71 58]\n",
            " [62 19 44 23 81 54 25 81 23 25]\n",
            " [74 44 61 23 19 70 23 25 23 72]\n",
            " [62 23 81 54 74 23 65 54 31 74]\n",
            " [23 62 25 46 23 54 74 70 23 81]\n",
            " [65 43 62 62 31 19 44 23 25 44]\n",
            " [23 78 44 44 25 23 54 25 61 23]\n",
            " [45 49 48 19 44 62 75 59 73 23]]\n",
            "\n",
            "y\n",
            " [[54 25 18 81 74 70 23 71 58 58]\n",
            " [19 44 23 81 54 25 81 23 25 81]\n",
            " [44 61 23 19 70 23 25 23 72 19]\n",
            " [23 81 54 74 23 65 54 31 74 72]\n",
            " [62 25 46 23 54 74 70 23 81 74]\n",
            " [43 62 62 31 19 44 23 25 44 61]\n",
            " [78 44 44 25 23 54 25 61 23 62]\n",
            " [49 48 19 44 62 75 59 73 23 80]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do57P9dtnQBN",
        "colab_type": "text"
      },
      "source": [
        "### Defining the network with PyTorch\n",
        "Model Structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZyRwutPU22j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3883cf0a-9098-4023-ad3c-1bdf35d7c43a"
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wcger9iDNu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV2VTY9bGNdp",
        "colab_type": "text"
      },
      "source": [
        "Time to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkWMWkP1EdIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RotfRC3MJxgo",
        "colab_type": "text"
      },
      "source": [
        "Instantiating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqDnsSV1Jtvc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "69339512-e86e-4f26-c41e-f21f76a3eab9"
      },
      "source": [
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygA8ROaSJ5O2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6106b6ad-d9c4-4c35-cdf5-5e10c116ad82"
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.2618... Val Loss: 3.1960\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1475... Val Loss: 3.1330\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1435... Val Loss: 3.1220\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1156... Val Loss: 3.1184\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1398... Val Loss: 3.1169\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1170... Val Loss: 3.1146\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1052... Val Loss: 3.1120\n",
            "Epoch: 1/20... Step: 80... Loss: 3.1157... Val Loss: 3.1042\n",
            "Epoch: 1/20... Step: 90... Loss: 3.1037... Val Loss: 3.0857\n",
            "Epoch: 1/20... Step: 100... Loss: 3.0630... Val Loss: 3.0490\n",
            "Epoch: 1/20... Step: 110... Loss: 2.9998... Val Loss: 2.9731\n",
            "Epoch: 1/20... Step: 120... Loss: 2.8543... Val Loss: 2.8926\n",
            "Epoch: 1/20... Step: 130... Loss: 2.8270... Val Loss: 2.7789\n",
            "Epoch: 2/20... Step: 140... Loss: 2.7190... Val Loss: 2.6726\n",
            "Epoch: 2/20... Step: 150... Loss: 2.6346... Val Loss: 2.5805\n",
            "Epoch: 2/20... Step: 160... Loss: 2.5563... Val Loss: 2.5089\n",
            "Epoch: 2/20... Step: 170... Loss: 2.4739... Val Loss: 2.4606\n",
            "Epoch: 2/20... Step: 180... Loss: 2.4516... Val Loss: 2.4255\n",
            "Epoch: 2/20... Step: 190... Loss: 2.4063... Val Loss: 2.3945\n",
            "Epoch: 2/20... Step: 200... Loss: 2.3944... Val Loss: 2.3794\n",
            "Epoch: 2/20... Step: 210... Loss: 2.3755... Val Loss: 2.3407\n",
            "Epoch: 2/20... Step: 220... Loss: 2.3319... Val Loss: 2.3113\n",
            "Epoch: 2/20... Step: 230... Loss: 2.3135... Val Loss: 2.2816\n",
            "Epoch: 2/20... Step: 240... Loss: 2.2963... Val Loss: 2.2580\n",
            "Epoch: 2/20... Step: 250... Loss: 2.2290... Val Loss: 2.2299\n",
            "Epoch: 2/20... Step: 260... Loss: 2.2060... Val Loss: 2.2098\n",
            "Epoch: 2/20... Step: 270... Loss: 2.2164... Val Loss: 2.1890\n",
            "Epoch: 3/20... Step: 280... Loss: 2.2125... Val Loss: 2.1613\n",
            "Epoch: 3/20... Step: 290... Loss: 2.1785... Val Loss: 2.1391\n",
            "Epoch: 3/20... Step: 300... Loss: 2.1515... Val Loss: 2.1158\n",
            "Epoch: 3/20... Step: 310... Loss: 2.1203... Val Loss: 2.0974\n",
            "Epoch: 3/20... Step: 320... Loss: 2.0901... Val Loss: 2.0781\n",
            "Epoch: 3/20... Step: 330... Loss: 2.0697... Val Loss: 2.0612\n",
            "Epoch: 3/20... Step: 340... Loss: 2.0876... Val Loss: 2.0433\n",
            "Epoch: 3/20... Step: 350... Loss: 2.0672... Val Loss: 2.0274\n",
            "Epoch: 3/20... Step: 360... Loss: 2.0005... Val Loss: 2.0081\n",
            "Epoch: 3/20... Step: 370... Loss: 2.0272... Val Loss: 1.9919\n",
            "Epoch: 3/20... Step: 380... Loss: 2.0119... Val Loss: 1.9745\n",
            "Epoch: 3/20... Step: 390... Loss: 1.9856... Val Loss: 1.9634\n",
            "Epoch: 3/20... Step: 400... Loss: 1.9502... Val Loss: 1.9466\n",
            "Epoch: 3/20... Step: 410... Loss: 1.9633... Val Loss: 1.9348\n",
            "Epoch: 4/20... Step: 420... Loss: 1.9571... Val Loss: 1.9186\n",
            "Epoch: 4/20... Step: 430... Loss: 1.9452... Val Loss: 1.9043\n",
            "Epoch: 4/20... Step: 440... Loss: 1.9228... Val Loss: 1.8963\n",
            "Epoch: 4/20... Step: 450... Loss: 1.8621... Val Loss: 1.8801\n",
            "Epoch: 4/20... Step: 460... Loss: 1.8625... Val Loss: 1.8666\n",
            "Epoch: 4/20... Step: 470... Loss: 1.8898... Val Loss: 1.8607\n",
            "Epoch: 4/20... Step: 480... Loss: 1.8686... Val Loss: 1.8445\n",
            "Epoch: 4/20... Step: 490... Loss: 1.8754... Val Loss: 1.8347\n",
            "Epoch: 4/20... Step: 500... Loss: 1.8679... Val Loss: 1.8216\n",
            "Epoch: 4/20... Step: 510... Loss: 1.8377... Val Loss: 1.8122\n",
            "Epoch: 4/20... Step: 520... Loss: 1.8644... Val Loss: 1.8036\n",
            "Epoch: 4/20... Step: 530... Loss: 1.8096... Val Loss: 1.7968\n",
            "Epoch: 4/20... Step: 540... Loss: 1.7768... Val Loss: 1.7874\n",
            "Epoch: 4/20... Step: 550... Loss: 1.8275... Val Loss: 1.7731\n",
            "Epoch: 5/20... Step: 560... Loss: 1.7951... Val Loss: 1.7692\n",
            "Epoch: 5/20... Step: 570... Loss: 1.7765... Val Loss: 1.7540\n",
            "Epoch: 5/20... Step: 580... Loss: 1.7509... Val Loss: 1.7452\n",
            "Epoch: 5/20... Step: 590... Loss: 1.7572... Val Loss: 1.7351\n",
            "Epoch: 5/20... Step: 600... Loss: 1.7478... Val Loss: 1.7280\n",
            "Epoch: 5/20... Step: 610... Loss: 1.7334... Val Loss: 1.7217\n",
            "Epoch: 5/20... Step: 620... Loss: 1.7279... Val Loss: 1.7118\n",
            "Epoch: 5/20... Step: 630... Loss: 1.7446... Val Loss: 1.7072\n",
            "Epoch: 5/20... Step: 640... Loss: 1.7133... Val Loss: 1.7002\n",
            "Epoch: 5/20... Step: 650... Loss: 1.7063... Val Loss: 1.6935\n",
            "Epoch: 5/20... Step: 660... Loss: 1.6830... Val Loss: 1.6883\n",
            "Epoch: 5/20... Step: 670... Loss: 1.7072... Val Loss: 1.6822\n",
            "Epoch: 5/20... Step: 680... Loss: 1.6975... Val Loss: 1.6768\n",
            "Epoch: 5/20... Step: 690... Loss: 1.6810... Val Loss: 1.6705\n",
            "Epoch: 6/20... Step: 700... Loss: 1.6734... Val Loss: 1.6596\n",
            "Epoch: 6/20... Step: 710... Loss: 1.6583... Val Loss: 1.6547\n",
            "Epoch: 6/20... Step: 720... Loss: 1.6581... Val Loss: 1.6464\n",
            "Epoch: 6/20... Step: 730... Loss: 1.6745... Val Loss: 1.6420\n",
            "Epoch: 6/20... Step: 740... Loss: 1.6345... Val Loss: 1.6337\n",
            "Epoch: 6/20... Step: 750... Loss: 1.6194... Val Loss: 1.6275\n",
            "Epoch: 6/20... Step: 760... Loss: 1.6532... Val Loss: 1.6254\n",
            "Epoch: 6/20... Step: 770... Loss: 1.6364... Val Loss: 1.6201\n",
            "Epoch: 6/20... Step: 780... Loss: 1.6284... Val Loss: 1.6134\n",
            "Epoch: 6/20... Step: 790... Loss: 1.6152... Val Loss: 1.6113\n",
            "Epoch: 6/20... Step: 800... Loss: 1.6151... Val Loss: 1.6050\n",
            "Epoch: 6/20... Step: 810... Loss: 1.6071... Val Loss: 1.6015\n",
            "Epoch: 6/20... Step: 820... Loss: 1.5748... Val Loss: 1.5966\n",
            "Epoch: 6/20... Step: 830... Loss: 1.6178... Val Loss: 1.5901\n",
            "Epoch: 7/20... Step: 840... Loss: 1.5715... Val Loss: 1.5873\n",
            "Epoch: 7/20... Step: 850... Loss: 1.5890... Val Loss: 1.5820\n",
            "Epoch: 7/20... Step: 860... Loss: 1.5655... Val Loss: 1.5725\n",
            "Epoch: 7/20... Step: 870... Loss: 1.5847... Val Loss: 1.5702\n",
            "Epoch: 7/20... Step: 880... Loss: 1.5853... Val Loss: 1.5675\n",
            "Epoch: 7/20... Step: 890... Loss: 1.5771... Val Loss: 1.5644\n",
            "Epoch: 7/20... Step: 900... Loss: 1.5582... Val Loss: 1.5582\n",
            "Epoch: 7/20... Step: 910... Loss: 1.5288... Val Loss: 1.5553\n",
            "Epoch: 7/20... Step: 920... Loss: 1.5573... Val Loss: 1.5512\n",
            "Epoch: 7/20... Step: 930... Loss: 1.5380... Val Loss: 1.5473\n",
            "Epoch: 7/20... Step: 940... Loss: 1.5395... Val Loss: 1.5451\n",
            "Epoch: 7/20... Step: 950... Loss: 1.5544... Val Loss: 1.5392\n",
            "Epoch: 7/20... Step: 960... Loss: 1.5584... Val Loss: 1.5339\n",
            "Epoch: 7/20... Step: 970... Loss: 1.5598... Val Loss: 1.5318\n",
            "Epoch: 8/20... Step: 980... Loss: 1.5246... Val Loss: 1.5314\n",
            "Epoch: 8/20... Step: 990... Loss: 1.5348... Val Loss: 1.5253\n",
            "Epoch: 8/20... Step: 1000... Loss: 1.5213... Val Loss: 1.5169\n",
            "Epoch: 8/20... Step: 1010... Loss: 1.5608... Val Loss: 1.5191\n",
            "Epoch: 8/20... Step: 1020... Loss: 1.5287... Val Loss: 1.5150\n",
            "Epoch: 8/20... Step: 1030... Loss: 1.5083... Val Loss: 1.5129\n",
            "Epoch: 8/20... Step: 1040... Loss: 1.5279... Val Loss: 1.5136\n",
            "Epoch: 8/20... Step: 1050... Loss: 1.4995... Val Loss: 1.5043\n",
            "Epoch: 8/20... Step: 1060... Loss: 1.5026... Val Loss: 1.5041\n",
            "Epoch: 8/20... Step: 1070... Loss: 1.5105... Val Loss: 1.4964\n",
            "Epoch: 8/20... Step: 1080... Loss: 1.5054... Val Loss: 1.4964\n",
            "Epoch: 8/20... Step: 1090... Loss: 1.4853... Val Loss: 1.4966\n",
            "Epoch: 8/20... Step: 1100... Loss: 1.4819... Val Loss: 1.4876\n",
            "Epoch: 8/20... Step: 1110... Loss: 1.4871... Val Loss: 1.4877\n",
            "Epoch: 9/20... Step: 1120... Loss: 1.4958... Val Loss: 1.4910\n",
            "Epoch: 9/20... Step: 1130... Loss: 1.4898... Val Loss: 1.4854\n",
            "Epoch: 9/20... Step: 1140... Loss: 1.4886... Val Loss: 1.4761\n",
            "Epoch: 9/20... Step: 1150... Loss: 1.5075... Val Loss: 1.4774\n",
            "Epoch: 9/20... Step: 1160... Loss: 1.4592... Val Loss: 1.4729\n",
            "Epoch: 9/20... Step: 1170... Loss: 1.4626... Val Loss: 1.4714\n",
            "Epoch: 9/20... Step: 1180... Loss: 1.4605... Val Loss: 1.4717\n",
            "Epoch: 9/20... Step: 1190... Loss: 1.4920... Val Loss: 1.4646\n",
            "Epoch: 9/20... Step: 1200... Loss: 1.4343... Val Loss: 1.4687\n",
            "Epoch: 9/20... Step: 1210... Loss: 1.4629... Val Loss: 1.4618\n",
            "Epoch: 9/20... Step: 1220... Loss: 1.4555... Val Loss: 1.4592\n",
            "Epoch: 9/20... Step: 1230... Loss: 1.4363... Val Loss: 1.4562\n",
            "Epoch: 9/20... Step: 1240... Loss: 1.4353... Val Loss: 1.4518\n",
            "Epoch: 9/20... Step: 1250... Loss: 1.4568... Val Loss: 1.4492\n",
            "Epoch: 10/20... Step: 1260... Loss: 1.4555... Val Loss: 1.4521\n",
            "Epoch: 10/20... Step: 1270... Loss: 1.4487... Val Loss: 1.4465\n",
            "Epoch: 10/20... Step: 1280... Loss: 1.4610... Val Loss: 1.4406\n",
            "Epoch: 10/20... Step: 1290... Loss: 1.4470... Val Loss: 1.4454\n",
            "Epoch: 10/20... Step: 1300... Loss: 1.4339... Val Loss: 1.4397\n",
            "Epoch: 10/20... Step: 1310... Loss: 1.4458... Val Loss: 1.4387\n",
            "Epoch: 10/20... Step: 1320... Loss: 1.4087... Val Loss: 1.4421\n",
            "Epoch: 10/20... Step: 1330... Loss: 1.4140... Val Loss: 1.4357\n",
            "Epoch: 10/20... Step: 1340... Loss: 1.4065... Val Loss: 1.4369\n",
            "Epoch: 10/20... Step: 1350... Loss: 1.3962... Val Loss: 1.4301\n",
            "Epoch: 10/20... Step: 1360... Loss: 1.4062... Val Loss: 1.4281\n",
            "Epoch: 10/20... Step: 1370... Loss: 1.3916... Val Loss: 1.4280\n",
            "Epoch: 10/20... Step: 1380... Loss: 1.4318... Val Loss: 1.4233\n",
            "Epoch: 10/20... Step: 1390... Loss: 1.4437... Val Loss: 1.4219\n",
            "Epoch: 11/20... Step: 1400... Loss: 1.4447... Val Loss: 1.4236\n",
            "Epoch: 11/20... Step: 1410... Loss: 1.4615... Val Loss: 1.4224\n",
            "Epoch: 11/20... Step: 1420... Loss: 1.4423... Val Loss: 1.4180\n",
            "Epoch: 11/20... Step: 1430... Loss: 1.4047... Val Loss: 1.4190\n",
            "Epoch: 11/20... Step: 1440... Loss: 1.4276... Val Loss: 1.4135\n",
            "Epoch: 11/20... Step: 1450... Loss: 1.3639... Val Loss: 1.4131\n",
            "Epoch: 11/20... Step: 1460... Loss: 1.3987... Val Loss: 1.4134\n",
            "Epoch: 11/20... Step: 1470... Loss: 1.3794... Val Loss: 1.4102\n",
            "Epoch: 11/20... Step: 1480... Loss: 1.4025... Val Loss: 1.4093\n",
            "Epoch: 11/20... Step: 1490... Loss: 1.3842... Val Loss: 1.4065\n",
            "Epoch: 11/20... Step: 1500... Loss: 1.3704... Val Loss: 1.4069\n",
            "Epoch: 11/20... Step: 1510... Loss: 1.3606... Val Loss: 1.4045\n",
            "Epoch: 11/20... Step: 1520... Loss: 1.3932... Val Loss: 1.4005\n",
            "Epoch: 12/20... Step: 1530... Loss: 1.4446... Val Loss: 1.4000\n",
            "Epoch: 12/20... Step: 1540... Loss: 1.3942... Val Loss: 1.3986\n",
            "Epoch: 12/20... Step: 1550... Loss: 1.4056... Val Loss: 1.3971\n",
            "Epoch: 12/20... Step: 1560... Loss: 1.4101... Val Loss: 1.3925\n",
            "Epoch: 12/20... Step: 1570... Loss: 1.3647... Val Loss: 1.3971\n",
            "Epoch: 12/20... Step: 1580... Loss: 1.3439... Val Loss: 1.3924\n",
            "Epoch: 12/20... Step: 1590... Loss: 1.3327... Val Loss: 1.3945\n",
            "Epoch: 12/20... Step: 1600... Loss: 1.3652... Val Loss: 1.3904\n",
            "Epoch: 12/20... Step: 1610... Loss: 1.3579... Val Loss: 1.3920\n",
            "Epoch: 12/20... Step: 1620... Loss: 1.3563... Val Loss: 1.3884\n",
            "Epoch: 12/20... Step: 1630... Loss: 1.3771... Val Loss: 1.3905\n",
            "Epoch: 12/20... Step: 1640... Loss: 1.3543... Val Loss: 1.3868\n",
            "Epoch: 12/20... Step: 1650... Loss: 1.3310... Val Loss: 1.3854\n",
            "Epoch: 12/20... Step: 1660... Loss: 1.3882... Val Loss: 1.3819\n",
            "Epoch: 13/20... Step: 1670... Loss: 1.3492... Val Loss: 1.3910\n",
            "Epoch: 13/20... Step: 1680... Loss: 1.3757... Val Loss: 1.3818\n",
            "Epoch: 13/20... Step: 1690... Loss: 1.3465... Val Loss: 1.3790\n",
            "Epoch: 13/20... Step: 1700... Loss: 1.3416... Val Loss: 1.3742\n",
            "Epoch: 13/20... Step: 1710... Loss: 1.3196... Val Loss: 1.3766\n",
            "Epoch: 13/20... Step: 1720... Loss: 1.3342... Val Loss: 1.3738\n",
            "Epoch: 13/20... Step: 1730... Loss: 1.3779... Val Loss: 1.3735\n",
            "Epoch: 13/20... Step: 1740... Loss: 1.3409... Val Loss: 1.3733\n",
            "Epoch: 13/20... Step: 1750... Loss: 1.3049... Val Loss: 1.3767\n",
            "Epoch: 13/20... Step: 1760... Loss: 1.3328... Val Loss: 1.3700\n",
            "Epoch: 13/20... Step: 1770... Loss: 1.3609... Val Loss: 1.3733\n",
            "Epoch: 13/20... Step: 1780... Loss: 1.3313... Val Loss: 1.3682\n",
            "Epoch: 13/20... Step: 1790... Loss: 1.3236... Val Loss: 1.3659\n",
            "Epoch: 13/20... Step: 1800... Loss: 1.3389... Val Loss: 1.3638\n",
            "Epoch: 14/20... Step: 1810... Loss: 1.3431... Val Loss: 1.3743\n",
            "Epoch: 14/20... Step: 1820... Loss: 1.3283... Val Loss: 1.3649\n",
            "Epoch: 14/20... Step: 1830... Loss: 1.3491... Val Loss: 1.3622\n",
            "Epoch: 14/20... Step: 1840... Loss: 1.2991... Val Loss: 1.3603\n",
            "Epoch: 14/20... Step: 1850... Loss: 1.2788... Val Loss: 1.3598\n",
            "Epoch: 14/20... Step: 1860... Loss: 1.3372... Val Loss: 1.3588\n",
            "Epoch: 14/20... Step: 1870... Loss: 1.3488... Val Loss: 1.3576\n",
            "Epoch: 14/20... Step: 1880... Loss: 1.3311... Val Loss: 1.3589\n",
            "Epoch: 14/20... Step: 1890... Loss: 1.3559... Val Loss: 1.3603\n",
            "Epoch: 14/20... Step: 1900... Loss: 1.3260... Val Loss: 1.3575\n",
            "Epoch: 14/20... Step: 1910... Loss: 1.3325... Val Loss: 1.3581\n",
            "Epoch: 14/20... Step: 1920... Loss: 1.3174... Val Loss: 1.3541\n",
            "Epoch: 14/20... Step: 1930... Loss: 1.2847... Val Loss: 1.3519\n",
            "Epoch: 14/20... Step: 1940... Loss: 1.3499... Val Loss: 1.3508\n",
            "Epoch: 15/20... Step: 1950... Loss: 1.3221... Val Loss: 1.3517\n",
            "Epoch: 15/20... Step: 1960... Loss: 1.3154... Val Loss: 1.3494\n",
            "Epoch: 15/20... Step: 1970... Loss: 1.3114... Val Loss: 1.3514\n",
            "Epoch: 15/20... Step: 1980... Loss: 1.3087... Val Loss: 1.3489\n",
            "Epoch: 15/20... Step: 1990... Loss: 1.2957... Val Loss: 1.3471\n",
            "Epoch: 15/20... Step: 2000... Loss: 1.2898... Val Loss: 1.3471\n",
            "Epoch: 15/20... Step: 2010... Loss: 1.3111... Val Loss: 1.3445\n",
            "Epoch: 15/20... Step: 2020... Loss: 1.3205... Val Loss: 1.3466\n",
            "Epoch: 15/20... Step: 2030... Loss: 1.2971... Val Loss: 1.3480\n",
            "Epoch: 15/20... Step: 2040... Loss: 1.3155... Val Loss: 1.3443\n",
            "Epoch: 15/20... Step: 2050... Loss: 1.2942... Val Loss: 1.3427\n",
            "Epoch: 15/20... Step: 2060... Loss: 1.3044... Val Loss: 1.3396\n",
            "Epoch: 15/20... Step: 2070... Loss: 1.3123... Val Loss: 1.3407\n",
            "Epoch: 15/20... Step: 2080... Loss: 1.2955... Val Loss: 1.3424\n",
            "Epoch: 16/20... Step: 2090... Loss: 1.3003... Val Loss: 1.3369\n",
            "Epoch: 16/20... Step: 2100... Loss: 1.2909... Val Loss: 1.3364\n",
            "Epoch: 16/20... Step: 2110... Loss: 1.2819... Val Loss: 1.3383\n",
            "Epoch: 16/20... Step: 2120... Loss: 1.3044... Val Loss: 1.3369\n",
            "Epoch: 16/20... Step: 2130... Loss: 1.2724... Val Loss: 1.3374\n",
            "Epoch: 16/20... Step: 2140... Loss: 1.2821... Val Loss: 1.3355\n",
            "Epoch: 16/20... Step: 2150... Loss: 1.3093... Val Loss: 1.3348\n",
            "Epoch: 16/20... Step: 2160... Loss: 1.2916... Val Loss: 1.3351\n",
            "Epoch: 16/20... Step: 2170... Loss: 1.2859... Val Loss: 1.3354\n",
            "Epoch: 16/20... Step: 2180... Loss: 1.2815... Val Loss: 1.3329\n",
            "Epoch: 16/20... Step: 2190... Loss: 1.2975... Val Loss: 1.3343\n",
            "Epoch: 16/20... Step: 2200... Loss: 1.2807... Val Loss: 1.3302\n",
            "Epoch: 16/20... Step: 2210... Loss: 1.2468... Val Loss: 1.3280\n",
            "Epoch: 16/20... Step: 2220... Loss: 1.2900... Val Loss: 1.3331\n",
            "Epoch: 17/20... Step: 2230... Loss: 1.2671... Val Loss: 1.3287\n",
            "Epoch: 17/20... Step: 2240... Loss: 1.2883... Val Loss: 1.3301\n",
            "Epoch: 17/20... Step: 2250... Loss: 1.2579... Val Loss: 1.3293\n",
            "Epoch: 17/20... Step: 2260... Loss: 1.2690... Val Loss: 1.3263\n",
            "Epoch: 17/20... Step: 2270... Loss: 1.2809... Val Loss: 1.3282\n",
            "Epoch: 17/20... Step: 2280... Loss: 1.2842... Val Loss: 1.3238\n",
            "Epoch: 17/20... Step: 2290... Loss: 1.2766... Val Loss: 1.3243\n",
            "Epoch: 17/20... Step: 2300... Loss: 1.2537... Val Loss: 1.3259\n",
            "Epoch: 17/20... Step: 2310... Loss: 1.2731... Val Loss: 1.3232\n",
            "Epoch: 17/20... Step: 2320... Loss: 1.2708... Val Loss: 1.3228\n",
            "Epoch: 17/20... Step: 2330... Loss: 1.2558... Val Loss: 1.3239\n",
            "Epoch: 17/20... Step: 2340... Loss: 1.2849... Val Loss: 1.3206\n",
            "Epoch: 17/20... Step: 2350... Loss: 1.2801... Val Loss: 1.3192\n",
            "Epoch: 17/20... Step: 2360... Loss: 1.2867... Val Loss: 1.3200\n",
            "Epoch: 18/20... Step: 2370... Loss: 1.2583... Val Loss: 1.3180\n",
            "Epoch: 18/20... Step: 2380... Loss: 1.2583... Val Loss: 1.3192\n",
            "Epoch: 18/20... Step: 2390... Loss: 1.2695... Val Loss: 1.3226\n",
            "Epoch: 18/20... Step: 2400... Loss: 1.2893... Val Loss: 1.3181\n",
            "Epoch: 18/20... Step: 2410... Loss: 1.2744... Val Loss: 1.3179\n",
            "Epoch: 18/20... Step: 2420... Loss: 1.2554... Val Loss: 1.3154\n",
            "Epoch: 18/20... Step: 2430... Loss: 1.2730... Val Loss: 1.3152\n",
            "Epoch: 18/20... Step: 2440... Loss: 1.2469... Val Loss: 1.3148\n",
            "Epoch: 18/20... Step: 2450... Loss: 1.2514... Val Loss: 1.3135\n",
            "Epoch: 18/20... Step: 2460... Loss: 1.2613... Val Loss: 1.3147\n",
            "Epoch: 18/20... Step: 2470... Loss: 1.2480... Val Loss: 1.3145\n",
            "Epoch: 18/20... Step: 2480... Loss: 1.2630... Val Loss: 1.3137\n",
            "Epoch: 18/20... Step: 2490... Loss: 1.2412... Val Loss: 1.3114\n",
            "Epoch: 18/20... Step: 2500... Loss: 1.2404... Val Loss: 1.3090\n",
            "Epoch: 19/20... Step: 2510... Loss: 1.2591... Val Loss: 1.3138\n",
            "Epoch: 19/20... Step: 2520... Loss: 1.2659... Val Loss: 1.3118\n",
            "Epoch: 19/20... Step: 2530... Loss: 1.2726... Val Loss: 1.3130\n",
            "Epoch: 19/20... Step: 2540... Loss: 1.2821... Val Loss: 1.3096\n",
            "Epoch: 19/20... Step: 2550... Loss: 1.2437... Val Loss: 1.3121\n",
            "Epoch: 19/20... Step: 2560... Loss: 1.2617... Val Loss: 1.3050\n",
            "Epoch: 19/20... Step: 2570... Loss: 1.2471... Val Loss: 1.3085\n",
            "Epoch: 19/20... Step: 2580... Loss: 1.2734... Val Loss: 1.3101\n",
            "Epoch: 19/20... Step: 2590... Loss: 1.2378... Val Loss: 1.3064\n",
            "Epoch: 19/20... Step: 2600... Loss: 1.2390... Val Loss: 1.3070\n",
            "Epoch: 19/20... Step: 2610... Loss: 1.2501... Val Loss: 1.3080\n",
            "Epoch: 19/20... Step: 2620... Loss: 1.2255... Val Loss: 1.3073\n",
            "Epoch: 19/20... Step: 2630... Loss: 1.2351... Val Loss: 1.3041\n",
            "Epoch: 19/20... Step: 2640... Loss: 1.2594... Val Loss: 1.3051\n",
            "Epoch: 20/20... Step: 2650... Loss: 1.2533... Val Loss: 1.3087\n",
            "Epoch: 20/20... Step: 2660... Loss: 1.2515... Val Loss: 1.3043\n",
            "Epoch: 20/20... Step: 2670... Loss: 1.2580... Val Loss: 1.3051\n",
            "Epoch: 20/20... Step: 2680... Loss: 1.2498... Val Loss: 1.3037\n",
            "Epoch: 20/20... Step: 2690... Loss: 1.2465... Val Loss: 1.3061\n",
            "Epoch: 20/20... Step: 2700... Loss: 1.2491... Val Loss: 1.3017\n",
            "Epoch: 20/20... Step: 2710... Loss: 1.2229... Val Loss: 1.3027\n",
            "Epoch: 20/20... Step: 2720... Loss: 1.2228... Val Loss: 1.3056\n",
            "Epoch: 20/20... Step: 2730... Loss: 1.2110... Val Loss: 1.3014\n",
            "Epoch: 20/20... Step: 2740... Loss: 1.2142... Val Loss: 1.3028\n",
            "Epoch: 20/20... Step: 2750... Loss: 1.2270... Val Loss: 1.3033\n",
            "Epoch: 20/20... Step: 2760... Loss: 1.2194... Val Loss: 1.3032\n",
            "Epoch: 20/20... Step: 2770... Loss: 1.2556... Val Loss: 1.2991\n",
            "Epoch: 20/20... Step: 2780... Loss: 1.2838... Val Loss: 1.2994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxFZRxk3LYbB",
        "colab_type": "text"
      },
      "source": [
        "Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo6DYbZxKFCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqkFPncJMJBQ",
        "colab_type": "text"
      },
      "source": [
        "Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbM1-porMHzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-YrSWmtNcXU",
        "colab_type": "text"
      },
      "source": [
        "Priming and generating text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEnSEosRNYQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNHgmJH3N3Vq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "69d45f1b-1b9f-4a88-af24-732d8069f6cf"
      },
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna was strength on its laughter\n",
            "and him.\n",
            "\n",
            "\"Well, what have your face, and there's no\n",
            "house of the collar was there, and\n",
            "I'm gratiful to me?...\"\n",
            "\n",
            "And seeing Levin and\n",
            "her life at one of\n",
            "the croad, the marriage with her eyes. He was said as the commitse face\n",
            "on his study was asked as she could not trink take him a colver and still things.\n",
            "\n",
            "\"There was in a little\n",
            "forest to the sole princess, and the same as he saw is a\n",
            "grist and are, why they was satisfied at\n",
            "her since that this step as stopping in the chair were talking of in it the principle to a station of his first. All that were, he was so truck at it. But that would be so fact in the feelings in which I should be singered in his hands, and she was so life and saying about it. He has been angry into take them in a sort of privious sense. And he well\n",
            "and mistaken. The day he\n",
            "was at the princess, to be\n",
            "satisfied is now struggling. He came to the cause afder all the children. But\n",
            "two wife they say that\n",
            "she's saying to the matter and his ha\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kZBk_yEOJOp",
        "colab_type": "text"
      },
      "source": [
        "Loading a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm-uJ8BLN8iR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "552c0e4e-7f68-4516-9d0d-9b888e9d4b6f"
      },
      "source": [
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
        "with open('rnn_20_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twLe3UG2OMjo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "outputId": "f18ee509-6ac8-4542-bd54-f3abb316ad3f"
      },
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And Levin said to himself.\n",
            "\n",
            "\"What is\n",
            "it was a longer. I can't be so still fallood, but the same say, happening to\n",
            "thinking\n",
            "of the maid. But he'd not a letter and dirtuges at the crup triushes in the conversation.\n",
            "\n",
            "\"Yes, and I went in\n",
            "were the man, too, to say this with the\n",
            "pretty. And I will lone to meet your less thoughts of more, to blame\n",
            "all the mirdly\n",
            "askants with you he would be simply as\n",
            "all\n",
            "that they see it. And I'm not chain of three people of the chair at the carriage of this meetings; but how did you know what I can't say what is not truting them. He has strong out on the man, and\n",
            "I can't believe her.\"\n",
            "\n",
            "Af the man was a cold and to the book.\n",
            "\n",
            "Alexey Alexandrovitch saw her\n",
            "short with the carryance. He was\n",
            "sporting in his book,\n",
            "the same side of wife and her his brack of the day of the\n",
            "previous part to\n",
            "her.\n",
            "\n",
            "\"Yes, there see somewhere? I'd been startfully. That's a picillary of\n",
            "the same.\" She wat handing his hands and without the sinner soul to the plict that there are not and already share from the\n",
            "distactes of this women, who had been a lady happiness and striking heart in silence, was asland.\n",
            "\n",
            "\"With a picture to be\n",
            "silled and seeing her. We have nothing to but this tretched. I shall gave me to believe it to the\n",
            "room. A comfinion of the mistress all a sourte of the same said that still men of my husband.\"\n",
            "\n",
            "Levin was still mare answered and drew in the coure. Anna dashed away the coming forth and the\n",
            "same arrive that to\n",
            "the sound of the same courtes, who was silent\n",
            "with the day of the country that he had been tried to have the same time the princess truthed off.\n",
            "\n",
            "\"I have\n",
            "been indeed,\" he\n",
            "said to Levin's face at the straight of the doctor, went out into the point of his\n",
            "whole, solution with a starting rooms women was\n",
            "an intense\n",
            "for her fire that\n",
            "with an honor had been doing that at once should see it all that the steps and the most than an efforts the marshals\n",
            "were saying, his brother was the sacistic of it, and was to she that she could be sincerely a sistara can talk, whom\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j-ypc10OQcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}